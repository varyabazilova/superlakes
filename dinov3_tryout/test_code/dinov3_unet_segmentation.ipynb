{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINOv3 + U-Net for Pixel-Level Lake Segmentation\n",
    "\n",
    "This notebook shows how to:\n",
    "1. Use DINOv3 as a feature extractor (frozen backbone)\n",
    "2. Add a U-Net decoder for pixel-level segmentation\n",
    "3. Train on your manual lake masks\n",
    "4. Get precise lake boundaries (not just patch-level predictions)\n",
    "\n",
    "**Key difference from previous approach:**\n",
    "- Previous: 224x224 patch → single prediction (\"has lake\")\n",
    "- This: 224x224 patch → 224x224 mask (pixel-level \"which pixels are lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varyabazilova/opt/anaconda3/envs/superlakes/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# DINOv3\n",
    "from transformers import Dinov2Model, Dinov2Config\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login (you already have this)\n",
    "login(token=\"my-login-token\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (657, 653, 4)\n",
      "Mask shape: (657, 653)\n"
     ]
    }
   ],
   "source": [
    "# Load your satellite image and mask\n",
    "image_path = \"/Users/varyabazilova/Desktop/glacial_lakes/super_lakes/dinov3_tryout/test_data/2021-09-04_fcc_testclip.tif\"\n",
    "mask_path = \"/Users/varyabazilova/Desktop/glacial_lakes/super_lakes/dinov3_tryout/test_data/lake_mask_testclip.tif\"\n",
    "\n",
    "with rasterio.open(image_path) as src:\n",
    "    image = src.read()  # Shape: (channels, height, width)\n",
    "    image = np.transpose(image, (1, 2, 0))  # Change to (height, width, channels)\n",
    "    \n",
    "with rasterio.open(mask_path) as src:\n",
    "    mask = src.read(1)  # Read first band\n",
    "\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "\n",
    "# Convert to 0-255 range for neural networks\n",
    "image_rgb = image[:,:,:3].astype(np.uint8)\n",
    "mask_binary = (mask > 0).astype(np.float32)  # Binary mask for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 16 patches\n"
     ]
    }
   ],
   "source": [
    "class LakeSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that creates image patches and corresponding mask patches\n",
    "    for training pixel-level segmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, image, mask, patch_size=224, max_patches=100):\n",
    "        self.image = image\n",
    "        self.mask = mask\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Create patches\n",
    "        self.patches = []\n",
    "        self.mask_patches = []\n",
    "        \n",
    "        height, width = image.shape[:2]\n",
    "        patch_count = 0\n",
    "        \n",
    "        # Sample patches across the image\n",
    "        for y in range(0, height-patch_size, patch_size//2):\n",
    "            for x in range(0, width-patch_size, patch_size//2):\n",
    "                if patch_count >= max_patches:\n",
    "                    break\n",
    "                    \n",
    "                img_patch = image[y:y+patch_size, x:x+patch_size]\n",
    "                mask_patch = mask[y:y+patch_size, x:x+patch_size]\n",
    "                \n",
    "                if img_patch.shape[:2] == (patch_size, patch_size):\n",
    "                    self.patches.append(img_patch)\n",
    "                    self.mask_patches.append(mask_patch)\n",
    "                    patch_count += 1\n",
    "            \n",
    "            if patch_count >= max_patches:\n",
    "                break\n",
    "        \n",
    "        print(f\"Created dataset with {len(self.patches)} patches\")\n",
    "        \n",
    "        # Data transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_patch = self.patches[idx]\n",
    "        mask_patch = self.mask_patches[idx]\n",
    "        \n",
    "        # Transform image\n",
    "        image_tensor = self.transform(image_patch)\n",
    "        \n",
    "        # Convert mask to tensor\n",
    "        mask_tensor = torch.from_numpy(mask_patch).float().unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "# Create dataset\n",
    "dataset = LakeSegmentationDataset(image_rgb, mask_binary, patch_size=224, max_patches=50)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define DINOv3 + U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: DINOv3 (frozen) + U-Net decoder (trainable)\n",
      "Model parameters: 5,090,177\n"
     ]
    }
   ],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple U-Net decoder that takes DINOv3 features and outputs pixel-level predictions\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=768, num_classes=1):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        \n",
    "        # Decoder layers - upsample from 14x14 to 224x224\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(feature_dim, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )  # 14x14 -> 28x28\n",
    "        \n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )  # 28x28 -> 56x56\n",
    "        \n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )  # 56x56 -> 112x112\n",
    "        \n",
    "        self.decoder4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )  # 112x112 -> 224x224\n",
    "        \n",
    "        # Final prediction layer\n",
    "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.decoder1(x)\n",
    "        x = self.decoder2(x)\n",
    "        x = self.decoder3(x)\n",
    "        x = self.decoder4(x)\n",
    "        x = self.final(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DINOv3UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete model: DINOv3 backbone + U-Net decoder for segmentation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DINOv3UNet, self).__init__()\n",
    "        \n",
    "        # Load DINOv3 model (smaller version for learning)\n",
    "        self.dinov3 = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "        \n",
    "        # Freeze DINOv3 parameters (use as feature extractor only)\n",
    "        for param in self.dinov3.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # U-Net decoder\n",
    "        self.decoder = UNetDecoder(feature_dim=768)  # DINOv2-base has 768 features\n",
    "        \n",
    "        print(\"Model created: DINOv3 (frozen) + U-Net decoder (trainable)\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features with DINOv3\n",
    "        with torch.no_grad():  # Don't compute gradients for DINOv3\n",
    "            features = self.dinov3(x).last_hidden_state  # Shape: (batch, 257, 768)\n",
    "            \n",
    "            # Remove CLS token and reshape to spatial format\n",
    "            patch_features = features[:, 1:]  # Remove first token (CLS), shape: (batch, 256, 768)\n",
    "            \n",
    "            # Reshape to 2D feature map: 256 patches = 16x16 grid for 224x224 input\n",
    "            batch_size = patch_features.shape[0]\n",
    "            feature_map = patch_features.reshape(batch_size, 16, 16, 768)\n",
    "            feature_map = feature_map.permute(0, 3, 1, 2)  # (batch, 768, 16, 16)\n",
    "        \n",
    "        # Generate segmentation mask\n",
    "        mask = self.decoder(feature_map)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "# Create model\n",
    "model = DINOv3UNet().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy for lake/no-lake\n",
    "optimizer = optim.Adam(model.decoder.parameters(), lr=0.001)  # Only train decoder\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([4, 1, 224, 224])) that is different to the input size (torch.Size([4, 1, 256, 256])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[39m\n\u001b[32m     14\u001b[39m outputs = model(images)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     20\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/superlakes/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/superlakes/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/superlakes/lib/python3.11/site-packages/torch/nn/modules/loss.py:697\u001b[39m, in \u001b[36mBCELoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    696\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/superlakes/lib/python3.11/site-packages/torch/nn/functional.py:3545\u001b[39m, in \u001b[36mbinary_cross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction)\u001b[39m\n\u001b[32m   3543\u001b[39m     reduction_enum = _Reduction.get_enum(reduction)\n\u001b[32m   3544\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target.size() != \u001b[38;5;28minput\u001b[39m.size():\n\u001b[32m-> \u001b[39m\u001b[32m3545\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3546\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is deprecated. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3547\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure they have the same size.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3548\u001b[39m     )\n\u001b[32m   3550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3551\u001b[39m     new_size = _infer_size(target.size(), weight.size())\n",
      "\u001b[31mValueError\u001b[39m: Using a target size (torch.Size([4, 1, 224, 224])) that is different to the input size (torch.Size([4, 1, 256, 256])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 5 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(dataloader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\\n')\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs=5)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a few examples\n",
    "model.eval()\n",
    "\n",
    "# Get a batch of test data\n",
    "test_images, test_masks = next(iter(dataloader))\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    predicted_masks = model(test_images)\n",
    "\n",
    "# Move to CPU for visualization\n",
    "test_images = test_images.cpu()\n",
    "test_masks = test_masks.cpu()\n",
    "predicted_masks = predicted_masks.cpu()\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "for i in range(min(4, len(test_images))):\n",
    "    # Original image (denormalize for display)\n",
    "    img = test_images[i].permute(1, 2, 0)\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    \n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f'Input Image {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Ground truth mask\n",
    "    axes[1, i].imshow(test_masks[i].squeeze(), cmap='Blues')\n",
    "    axes[1, i].set_title(f'Ground Truth {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Predicted mask\n",
    "    pred_mask = predicted_masks[i].squeeze()\n",
    "    axes[2, i].imshow(pred_mask, cmap='Reds')\n",
    "    axes[2, i].set_title(f'Prediction {i+1}')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "def calculate_metrics(pred_masks, true_masks, threshold=0.5):\n",
    "    pred_binary = (pred_masks > threshold).float()\n",
    "    \n",
    "    intersection = (pred_binary * true_masks).sum()\n",
    "    union = pred_binary.sum() + true_masks.sum() - intersection\n",
    "    \n",
    "    iou = intersection / (union + 1e-8)\n",
    "    accuracy = ((pred_binary == true_masks).float().mean())\n",
    "    \n",
    "    return iou.item(), accuracy.item()\n",
    "\n",
    "iou, accuracy = calculate_metrics(predicted_masks, test_masks)\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"IoU (Intersection over Union): {iou:.3f}\")\n",
    "print(f\"Pixel Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Apply to Full Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_full_image(model, image, patch_size=224, stride=112):\n",
    "    \"\"\"\n",
    "    Apply the trained model to a full large image using sliding window\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Create output mask\n",
    "    full_mask = np.zeros((height, width), dtype=np.float32)\n",
    "    count_mask = np.zeros((height, width), dtype=np.float32)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    print(\"Applying model to full image...\")\n",
    "    \n",
    "    patches_processed = 0\n",
    "    with torch.no_grad():\n",
    "        for y in range(0, height - patch_size + 1, stride):\n",
    "            for x in range(0, width - patch_size + 1, stride):\n",
    "                # Extract patch\n",
    "                patch = image[y:y+patch_size, x:x+patch_size]\n",
    "                \n",
    "                # Transform and predict\n",
    "                patch_tensor = transform(patch).unsqueeze(0).to(device)\n",
    "                pred_mask = model(patch_tensor).squeeze().cpu().numpy()\n",
    "                \n",
    "                # Add to full mask\n",
    "                full_mask[y:y+patch_size, x:x+patch_size] += pred_mask\n",
    "                count_mask[y:y+patch_size, x:x+patch_size] += 1\n",
    "                \n",
    "                patches_processed += 1\n",
    "                if patches_processed % 20 == 0:\n",
    "                    print(f\"  Processed {patches_processed} patches...\")\n",
    "    \n",
    "    # Average overlapping predictions\n",
    "    full_mask = np.divide(full_mask, count_mask, out=np.zeros_like(full_mask), where=count_mask!=0)\n",
    "    \n",
    "    return full_mask\n",
    "\n",
    "# Apply to a smaller region first (full image might be too large)\n",
    "# Take a 1000x1000 subset for demonstration\n",
    "subset_size = 1000\n",
    "y_start, x_start = 1000, 1000  # Adjust these coordinates\n",
    "image_subset = image_rgb[y_start:y_start+subset_size, x_start:x_start+subset_size]\n",
    "mask_subset = mask_binary[y_start:y_start+subset_size, x_start:x_start+subset_size]\n",
    "\n",
    "# Predict on subset\n",
    "predicted_full_mask = predict_full_image(model, image_subset, patch_size=224, stride=112)\n",
    "\n",
    "# Visualize full prediction\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(image_subset)\n",
    "axes[0].set_title('Original Image (Subset)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(mask_subset, cmap='Blues')\n",
    "axes[1].set_title('Ground Truth Mask')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(predicted_full_mask, cmap='Reds')\n",
    "axes[2].set_title('DINOv3+UNet Prediction')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPixel-level segmentation complete!\")\n",
    "print(\"This approach gives you precise lake boundaries, not just patch-level predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What this notebook achieved:**\n",
    "\n",
    "1. **True pixel-level segmentation** - each pixel gets classified as lake/not lake\n",
    "2. **DINOv3 features** - powerful satellite-trained representations \n",
    "3. **U-Net decoder** - specialized for precise boundary detection\n",
    "4. **Training on your data** - learns from your manual lake masks\n",
    "\n",
    "**Key advantages over patch classification:**\n",
    "- Precise lake boundaries (not rectangular patches)\n",
    "- Pixel-level accuracy\n",
    "- Better for tracking lake changes over time\n",
    "- Scalable to any image size\n",
    "\n",
    "**Next steps:**\n",
    "- Train on more data for better accuracy\n",
    "- Apply to multiple time periods to track changes\n",
    "- Fine-tune hyperparameters\n",
    "- Use larger DINOv3 models for better features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:superlakes] *",
   "language": "python",
   "name": "conda-env-superlakes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
